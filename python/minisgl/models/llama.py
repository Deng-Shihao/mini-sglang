from __future__ import annotations

from typing import TYPE_CHECKING, Optional, Tuple

import torch
from minisgl.core import get_global_ctx
from minisgl.layers import BaseOP, OPList, ParallelLMHead, RMSNormFused, VocabParallelEmbedding
from minisgl.layers.quantization import LinearMethodBase
from minisgl.utils import nvtx_annotate

from .base import BaseLLMModel
from .utils import GatedMLP as LlamaMLP
from .utils import RopeAttn as LlamaAttn

if TYPE_CHECKING:
    from .config import ModelConfig


class LlamaDecoderLayer(BaseOP):
    def __init__(
        self,
        config: ModelConfig,
        layer_id: int,
        linear_method: Optional[LinearMethodBase] = None,
        params_dtype: torch.dtype = torch.float16,
    ):
        self.self_attn = LlamaAttn(
            config,
            layer_id,
            linear_method=linear_method,
            params_dtype=params_dtype,
        )
        self.mlp = LlamaMLP(
            config,
            linear_method=linear_method,
            params_dtype=params_dtype,
        )
        self.input_layernorm = RMSNormFused(
            size=config.hidden_size,
            eps=config.rms_norm_eps,
        )
        self.post_attention_layernorm = RMSNormFused(
            size=config.hidden_size,
            eps=config.rms_norm_eps,
        )

        self._layer_id = layer_id

    @nvtx_annotate("Layer_{}", layer_id_field="_layer_id")
    def forward(
        self,
        x: torch.Tensor,
        residual: torch.Tensor | None = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        x, residual = self.input_layernorm.forward(x, residual)
        x = self.self_attn.forward(x)
        x, residual = self.post_attention_layernorm.forward(x, residual)
        x = self.mlp.forward(x)
        return x, residual


class LlamaModel(BaseOP):
    def __init__(
        self,
        config: ModelConfig,
        linear_method: Optional[LinearMethodBase] = None,
        params_dtype: torch.dtype = torch.float16,
    ):
        self.embed_tokens = VocabParallelEmbedding(
            num_embeddings=config.vocab_size,
            embedding_dim=config.hidden_size,
        )
        self.layers = OPList(
            [
                LlamaDecoderLayer(
                    config,
                    layer_id,
                    linear_method=linear_method,
                    params_dtype=params_dtype,
                )
                for layer_id in range(config.num_layers)
            ]
        )
        self.norm = RMSNormFused(
            size=config.hidden_size,
            eps=config.rms_norm_eps,
        )

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        x = self.embed_tokens.forward(input_ids)
        residual: torch.Tensor | None = None
        for layer in self.layers.op_list:
            x, residual = layer.forward(x, residual)
        return self.norm.forward(x, residual)[0]


class LlamaForCausalLM(BaseLLMModel):
    def __init__(
        self,
        config: ModelConfig,
        params_dtype: torch.dtype = torch.float16,
    ):
        # Get linear method from quantization config if present
        linear_method = None
        if config.quantization_config is not None:
            linear_method = config.quantization_config.get_linear_method()
        
        self.model = LlamaModel(
            config,
            linear_method=linear_method,
            params_dtype=params_dtype,
        )
        self.lm_head = ParallelLMHead(
            num_embeddings=config.vocab_size,
            embedding_dim=config.hidden_size,
            tie_word_embeddings=config.tie_word_embeddings,
            tied_embedding=self.model.embed_tokens if config.tie_word_embeddings else None,
        )
        super().__init__()

    def forward(self) -> torch.Tensor:
        output = self.model.forward(get_global_ctx().batch.input_ids)
        logits = self.lm_head.forward(output)
        return logits


__all__ = ["LlamaForCausalLM"]

